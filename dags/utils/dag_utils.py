from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models.baseoperator import chain
from simpot.serialize import mapper_all,  serialize_to_rdf, serialize_to_rdf_file

from utils.mongo import get_mongo_db, insert_many, drop_collection
import utils.consumers as consumers
import utils.models

import requests
import os

from airflow.models import Variable


# a partir de um dado, e de um mapper generico, retorna um mapper específico
def mapper_generate (obj, mapeamento):
    new_map = {}
    for collumn, list_collumn in mapeamento.items():
        for current in list_collumn:
            if current in obj.keys():
                new_map[collumn] = current
                break
    return new_map

def append_key_value (data, key, value):
    return list(map(lambda x: {**x, key: value}, data))

def save_content_to_file(file, content):
    print(f'[INFO] - Saving file {file}')
    with open(file, 'w') as f:
        f.write(content)


def extract (consumer, params):
    data = consumer.request(**params).to_dict('records')
    return data

def transform (data, gen_mapper, dbpedia_url):
    mapper = mapper_generate (data[0], gen_mapper)   
    data = mapper_all(mapper, data)
    return append_key_value (data, "instituicao", dbpedia_url)

def dynamic_elt(institute, collection, consumer, params, generic_mapper, dbpedia_url):
    data = extract (consumer, params)
    data = transform(data, generic_mapper[collection], dbpedia_url)
    insert_many(get_mongo_db(institute),collection,data)
    return f"Inserted {collection} in {institute} {data[0:100]}"
 

def dynamic_ttl (institute, collection, model_class):
    db = get_mongo_db(institute)
    mongo_collection = db[collection]
    documents = list(mongo_collection.find())
    content = serialize_to_rdf(documents, model_class)
    local_save = int(Variable.get("local_save", default_var=0))
    dw_save = int(Variable.get("dw_save", default_var=0))
    if local_save:
        filename = f"/opt/airflow/download/{institute}_{collection}.ttl"
        save_content_to_file(filename, content)
    if dw_save:
        dw_settings = Variable.get("dw_settings", deserialize_json=True)
        token = dw_settings["token"]
        dataset_name = dw_settings["dataset"]
        owner = dw_settings["owner"]
        send_content(dataset_name, owner, content, f'{institute}_{collection}.ttl', token)

    return {"ok": content[0:200] }


def send_content (name, owner, content, file_name, token):
    #url = f'https://api.data.world/v0/uploads/{owner}/{name}/files/{file_name}'
    print (owner)
    url = f'https://api.data.world/v0/uploads/{owner}/{name}/files/{file_name}'
    print (url)
    headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json',
            }

    
    response = requests.put(url, data=content.encode('utf-8'), headers=headers)

    if response.status_code == 200:
        print('Conteúdo do arquivo enviado com sucesso!')
    else:
        print(f'Erro {response.status_code}: {response.text} , {url}')


def create_dataset (name, owner, token):
    name = name
    description = name
    tags = []
    dataset = {
        'title': name,
        'description': description,
        'tags': tags,
        "visibility": "OPEN",
    }

    url = f'https://api.data.world/v0/datasets/{owner}'
    headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json',
            }
    response = requests.post(url, json=dataset, headers=headers)

    if response.status_code == 200:
        print('Conjunto de dados criado com sucesso!')
    else:
        print(f'Erro {response.status_code}: {response.text}')





#####################################################
###
####################################################

def dynamic_create_dag(dag_id:str, institute, conf, generic_mapper, schedule_interval, start_date, default_args):
    collections = conf["colecoes"]
    main_url = conf["main_url"]
    dbpedia_url = conf["dbpedia_pt"]
    extract_total = int (Variable.get("extract_total", default_var=10))
    consumer = getattr(consumers, conf['consumer']) (main_url, extract_total)
    

    dag = DAG(
        f'{dag_id}_etl',
        default_args=default_args,
        description=f'A simple DAG to extract data from {institute} API',
        schedule_interval=schedule_interval,
        start_date=start_date
    )


    drop_task = []
    for collection, paramns in collections.items():
        task = PythonOperator(
            task_id=f'drop_{collection}',
            python_callable=drop_collection,
            op_kwargs={'institute': institute, 'collection_name': collection},
            dag=dag,
        )
        drop_task.append(task)
    
    
    elt_task = []
    for collection, params in collections.items():
        task = PythonOperator(
            task_id=f'run_intake_{collection}',
            python_callable= dynamic_elt,
            op_kwargs={'institute':institute,'collection':collection, 'consumer': consumer, 'params':params, "generic_mapper":generic_mapper, "dbpedia_url":dbpedia_url},
            dag=dag,
        )
        elt_task.append(task)


    ttl_task = []

   
    for collection, params in collections.items():
        class_ = getattr(utils.models, collection.capitalize()) 
        oper = PythonOperator(
            task_id=f'transform_{collection}',
            python_callable= dynamic_ttl,
            op_kwargs={"institute":institute, "collection":collection, "model_class":class_},
            dag=dag,
        )
        ttl_task.append(oper)

    chain(drop_task, elt_task,ttl_task)
    
    
    return dag